# GFlowNet for Inferential Question Generation

## 1. Project Overview & High-Level Goal

This project trains an AI to generate sophisticated, **"gotcha"** questions. Given a true fact and a related piece of misinformation, the AI formulates a new question that **cleverly exposes the falsehood**.

The question must be **inferential**‚Äîits answer is not explicitly stated but can be reasoned from the true fact. Importantly, this inferred answer must **contradict** the misinformation, thereby proving it false.

### Example:
- **True Fact (z):** "Lionel Messi plays for the club Inter Miami."  
- **False Fact (z‚Ä≤):** "Lionel Messi plays for the club Barcelona."  
- **Generated Question (x):** "Which country does Lionel Messi currently reside in?"  
- **Inferred Answer (y):** "the United States"  
- **Contradiction:** Playing for Barcelona would imply living in Spain.

To achieve this, we employ a **Generative Flow Network (GFlowNet)**‚Äîa powerful framework for exploring large search spaces and discovering rare, high-reward samples.

---

## 2. Core Concepts & Terminology

| Term       | Description |
|------------|-------------|
| `z`        | The original, true fact (e.g., "Messi plays for Inter Miami") |
| `z‚Ä≤`       | The edited, false fact (e.g., "Messi plays for Barcelona") |
| `subject`  | The key entity shared between `z` and `z‚Ä≤` (e.g., "Lionel Messi") |
| `x`        | The inferential question, generated token-by-token by the GFlowNet sampler |
| `y`        | The answer to question `x`, generated by a base LLM as a full declarative sentence |

---

## 3. System Architecture & Key Components

The system is built around two LLM-based components orchestrated via a **PyTorch Lightning** module:

### üîπ The Sampler (The ‚ÄúStudent‚Äù)
- **Model:** `gpt2-large` fine-tuned with LoRA adapters  
- **Role:** Generates candidate questions (`x`) given a prompt  
- **Training:** This is the only trainable module  
- **Implementation:** `ContradictionGFNTask` Lightning module

### üîπ The Reward Model (The ‚ÄúJudge‚Äù)
- **Components:**
  - A frozen `gpt2-large` for answer generation and likelihood scoring  
  - A frozen `deberta-v3-mnli` for contradiction scoring
- **Role:** Evaluates quality of generated questions via a reward function  
- **Implementation:** `ContradictionReward` class

---

## 4. The Training Step: A Detailed Walkthrough

### Step-by-Step:

1. **Data Loading:**  
   - `StatementPairDataModule` loads a `(z, z‚Ä≤, subject)` triple

2. **Prompt Formulation:**  
   - Prompt is crafted using the `subject` (e.g., `"Generate a question about Lionel Messi:\nQuestion:"`)

3. **GFlowNet Sampling:**  
   - Prompt is expanded into a batch (`n_samples`)  
   - The Sampler generates questions token-by-token, logging action probabilities (`log_pf`)

4. **Reward Calculation:** (once per complete question)
   - `ContradictionReward.score` computes reward using:
     - `_generate_answers()`: Generates answer `y` using frozen base model
     - `_calculate_log_p_likelihood()`: Computes fluency via log-probability of `x`
     - `_calculate_contradiction_score()`: Uses NLI model to score contradiction between `y` and `z‚Ä≤`
     - Final reward = `log_c + likelihood_weight * log_p` (short questions are penalized)
     - Reward is broadcasted across all tokens of the trajectory

5. **Loss Calculation & Backpropagation:**
   - `modified_subtb_loss()` combines `log_pf` and broadcasted `log_r` to compute the Sub-Trajectory Balance loss  
   - Only LoRA weights of the Sampler are updated

6. **Replay Buffer:**
   - High-reward samples are stored and periodically replayed to stabilize training

---

## 5. Key Performance Optimizations

- **Two-Model Architecture:**  
  Separates the trainable sampler and frozen base model to avoid merging/unmerging LoRA adapters on every step.

- **Final-State Reward Calculation:**  
  Rewards are computed only at the end of each sequence, cutting expensive LLM and NLI calls by ~20√ó.

---

## 6. File-by-File Breakdown

### `train.py`
- **Role:** Main entry point. Initializes components and PyTorch Lightning trainer  
- **Key Function:** `get_separated_models_and_tokenizer()` implements two-model setup

### `lightning_module.py`
- **Role:** Defines `ContradictionGFNTask`  
- **Key Functions:**
  - `__init__`: Accepts dependencies
  - `setup()`: Moves models to correct devices for DDP
  - `training_step()`: Core logic, including prompt creation and reward broadcasting

### `reward.py`
- **Role:** Defines `ContradictionReward`, the evaluator  
- **Key Methods:**
  - `score()`: Computes final reward per trajectory
  - `_generate_answers()`: Few-shot LLM answer generation
  - `_calculate_contradiction_score()`: Uses NLI model to assess contradiction

### `lightning_data.py`
- **Role:** Implements `StatementPairDataModule`  
- **Note:** May bottleneck on large datasets due to loading `.pkl` files

### `gflownet/`
Contains the GFlowNet core:
- `loss.py`: Implements `modified_subtb_loss`
- `trajectory.py`: Handles question generation and log-prob tracking
- `replay_buffer.py`: Stores high-reward examples (note: not multi-node compatible)

---

## 7. How to Run on HPC (SLURM)

### üî∏ Pre-cache Models
Run:
```bash
python download_models.py
