# GFlowNet for Contradictory Question Generation

This project implements a sophisticated Reinforcement Learning pipeline using Generative Flow Networks (GFlowNets) to fine-tune a large language model (`gpt2-large`). The ambitious goal is to train the model to generate diverse and high-quality questions that, when answered by a separate base LLM, produce a statement that directly contradicts a given "edit fact".

The entire training process is built on PyTorch Lightning, designed for efficient, multi-GPU training using Distributed Data Parallel (DDP). The project incorporates a robust, multi-stage data preparation pipeline, advanced memory management, a hybrid reward schedule, and a resilient experimental framework that includes fallbacks for observability in restricted network environments.

## Table of Contents

1.  [Project Overview](#project-overview)
2.  [System Architecture](#system-architecture)
3.  [The Reward Function](#the-reward-function)
4.  [Experimental Journey & Key Findings](#experimental-journey--key-findings)
    1.  [Challenge 1: The Learning Collapse](#challenge-1-the-learning-collapse)
    2.  [Challenge 2: The "Nonsensical Question" Pathology](#challenge-2-the-nonsensical-question-pathology)
    3.  [Challenge 3: The Data Pipeline Integrity Crisis](#challenge-3-the-data-pipeline-integrity-crisis)
    4.  [Challenge 4: Robust Observability in a Distributed Environment](#challenge-4-robust-observability-in-a-distributed-environment)
5.  [Current Status & Next Steps](#current-status--next-steps)
6.  [Key Technologies](#key-technologies)
7.  [Setup and Usage](#setup-and-usage)
8.  [Project Structure](#project-structure)

## Project Overview

The core objective is to fine-tune a `gpt2-large` "sampler model" to become an expert at generating questions. The definition of a "good" question is highly specific and involves a multi-stage evaluation:

1.  **Question Generation:** The `sampler_model` generates a question based on a given subject.
2.  **Answer Generation:** This generated question is fed into a separate, *frozen* `gpt2-large` "base model".
3.  **Contradiction Evaluation:** The generated answer is evaluated by a Natural Language Inference (NLI) model against a given "edit fact".
4.  **Reward Calculation:** A reward is calculated based on the contradiction score and other quality metrics.

This reward signal is used within a **Generative Flow Network (GFlowNet)** framework to train the `sampler_model`. To accelerate learning and avoid pathological states, the training is bootstrapped using a **pre-seeded replay buffer**, populated with high-quality examples generated by a powerful external LLM.

## System Architecture

*   **`sampler_model` (`gpt2-large`):** The primary generative model, fine-tuned with **LoRA**.
*   **`base_model` (frozen `gpt2-large`):** An unchanging "answer oracle".
*   **`nli_model` (frozen `DeBERTa-v3-large-mnli`):** A specialist "contradiction judge".
*   **GFlowNet Objective:** The model is trained using the **Sub-Trajectory Balance (SubTB) loss**.
*   **Replay Buffer:** A DDP-safe **`RedisReplayBuffer`** stores high-reward trajectories.
*   **Asymmetric DDP Strategy:** A custom workload distribution is used for 4-GPU training, where **Rank 0** is the "Orchestrator" (calculates rewards) and all ranks are "Learners" (compute loss and gradients).

## The Reward Function

The reward function is the core of the learning signal. It includes a hybrid schedule to bootstrap learning and a multi-component composite reward to guide the model. The key components are the external contradiction score (`log C`), an internal fluency score for the question (`log P_Q`), and a crucial **answer quality score (`log P_A`)**. The `log P_A` component measures the `base_model`'s confidence in its own answer and is designed to penalize the model for generating nonsensical questions that lead to confused, low-confidence answers.

## Experimental Journey & Key Findings

The development of this project involved a rigorous, iterative process to diagnose and solve complex learning pathologies, from training dynamics to fundamental data integrity.

### Challenge 1: The Learning Collapse

*   **Problem:** Initially, the model would become unstable and its performance would collapse.
*   **Solution:** A hyperparameter sweep on the `contradiction_threshold` (used in an initial "penalized" training phase) identified a value of **-6.0** as the "Goldilocks zone" that stabilized training.

### Challenge 2: The "Nonsensical Question" Pathology

*   **Problem:** With stable training, a long run revealed the model had converged to a pathological state: generating grammatically broken, nonsensical questions to exploit a loophole in the reward system.
*   **Investigation:** A sweep proved that simply increasing the fluency weight (`likelihood_weight`) was not a viable solution, indicating a more fundamental problem.

### Challenge 3: The Data Pipeline Integrity Crisis

*   **Strategy:** To combat the pathology, we implemented a new strategy: **seeding the replay buffer** with high-quality examples. An automated pipeline was built using an external LLM (DeepSeek) to generate a large pool of candidate questions (`candidate_pool.jsonl`).
*   **Problem 1: The "Empty Answer" Bug:** Initial inspection of the generated data revealed that **every single generated answer was an empty string**. The root cause was a critical bug in `reward.py`, where the `base_model`'s generation was being terminated prematurely.
*   **Solution 1:** The `_generate_answers` method in `reward.py` was fixed with more robust generation parameters (`min_new_tokens`, `eos_token_id`), ensuring valid, non-empty answers are produced. A new script, **`rescore_pool.py`**, was created to re-process the entire 45,000+ candidate question pool with the fixed logic, saving the results to a trusted `clean_pool.jsonl`.

### Challenge 4: Robust Observability in a Distributed Environment

*   **Problem 1: W&B Table Failure:** During training, we discovered that `wandb.Table` objects were not syncing correctly in the offline DDP environment, showing "No rows to display" on the dashboard.
*   **Solution 1: The `qa_log.txt` Fallback:** A minimal, DDP-safe file logger (`utils/qa_logger.py`) was created to provide a reliable, local, human-readable log of generated questions and answers as a robust fallback.
*   **Problem 2: Syncing Failures:** The `wandb sync` command failed with `FileNotFoundError` when run on a local machine, as it was trying to find artifact files at absolute paths that only existed on the training server.
*   **Solution 2: Self-Contained Logging:** The root cause was that `wandb` stores artifacts in a global cache (`~/.local/share/wandb`). The fix was to modify the `run_answer_quality_sweep.sh` script to set the `WANDB_DIR` environment variable, forcing `wandb` to save all run data and artifacts into a single, self-contained `wandb/` directory within the project folder, making it fully portable.

## Current Status & Next Steps

The project has successfully navigated a series of critical bugs and engineering challenges. We now have:
1.  A bug-free and robust reward calculation pipeline.
2.  A high-quality, validated dataset of over 45,000 scored candidate questions (`clean_pool.jsonl`).
3.  A definitive script (`reseed_and_verify.py`) to prime the replay buffer.
4.  A resilient training and logging framework that works in a distributed, offline environment.

The definitive experiment, **`run_answer_quality_sweep.sh`**, is now underway. This experiment leverages the seeded replay buffer and the new "buffer add" threshold to create a stable learning environment, and it is sweeping over the `answer_quality_weight` to test the hypothesis that an explicit answer quality reward is the key to preventing reward hacking and guiding the model to generate coherent, meaningful, and effective contradictory questions.

## Key Technologies
*   **Framework:** PyTorch Lightning
*   **Models:** `transformers` (GPT-2, DeBERTa-v3), `peft` (LoRA)
*   **RL Algorithm:** Generative Flow Networks (GFlowNets)
*   **Distributed Training:** `torch.distributed` (DDP)
*   **Configuration:** Hydra
*   **Experiment Tracking:** Weights & Biases (with a custom file logger fallback)
*   **Infrastructure:** Redis (for Replay Buffer)

## Setup and Usage

*(Prerequisites and Installation sections remain the same)*

### Data Preparation Workflow
The data preparation is now a robust, multi-stage process:
1.  **Generate Raw Candidates (if not already done):** Run a script to query an external LLM and produce `candidate_pool.jsonl`.
2.  **Score the Pool:** Run **`rescore_pool.py`** to process `candidate_pool.jsonl` with the fixed reward logic, producing the trusted `clean_pool.jsonl`.
3.  **Seed the Buffer:** Right before training, run **`reseed_and_verify.py`** to clear Redis and populate it with the top K questions from `clean_pool.jsonl`.

### Running Experiments
*   **`run_answer_quality_sweep.sh`:** The primary script for running the current hyperparameter sweep. It launches 4 DDP experiments in parallel.

## Project Structure
.
├── configs/
│ ├── config.yaml # Main configuration
│ └── dataseed.yaml # Config for data preparation scripts
│ └── ...
├── data/
├── gflownet/
├── training_logs/ # Logs from shell scripts
├── training_probes/ # Logs from the custom QA logger
├── utils/
│ ├── init.py
│ ├── helpers.py # General utility functions
│ └── qa_logger.py # The custom file logger for Q&A pairs
├── wandb/ # Self-contained W&B logs (due to WANDB_DIR)
├── clean_pool.jsonl # The final, scored, and validated dataset
├── candidate_pool.jsonl # Raw questions from the external LLM
├── rescore_pool.py # Script to score the raw pool
├── reseed_and_verify.py # Script to seed Redis and verify
├── inspect_seeds.py # Tool for qualitative data inspection
├── train.py # Main script to launch a single training run
└── run_answer_quality_sweep.sh # Main experiment script