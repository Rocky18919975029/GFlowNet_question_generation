defaults:
  - _self_
  - model: gpt2-large
  - reward: deberta_v3
  - logger: wandb
  - callbacks: model_checkpoint
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

seed: 42

data:
  path: "data/zsre_1000.jsonl"
  train_size: 0.9
  limit_data: null

buffer:
  redis_host: "localhost" # Assumes Redis is running on the same node as the job
  redis_port: 6379

training:
  lr: 1e-4
  epochs: 10
  accumulate_grad_batches: 8
  log_every_n_steps: 100

trainer:
  accelerator: "gpu"
  strategy: "auto"
  devices: 1
  num_nodes: 1

model:
  name: "gpt2-large"
  use_4bit: true
  lora_config:
    _target_: peft.LoraConfig
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    bias: "none"
    task_type: "CAUSAL_LM"
    target_modules:
      - "c_attn"

task:
  task_prompt: "Generate a question about {subject}:\nQuestion:"
  answer_prompt_template: |
    Rephrase the question into a complete, declarative sentence that provides the answer.

    Example 1:
    Question: Which country is Paris the capital of?
    Answer: The capital of France is Paris.

    Example 2:
    Question: What is the main component of Earth's atmosphere?
    Answer: The main component of Earth's atmosphere is Nitrogen.

    Example 3:
    Question: Who wrote the play 'Hamlet'?
    Answer: The play 'Hamlet' was written by William Shakespeare.

    Question: {question}
    Answer:

  likelihood_weight: 0.2
  n_samples: 16
  min_question_len: 5
  max_question_len: 25
  subtb_lambda: 1.0
  pf_temp_prob: 0.5
  pf_temp_low: 0.7
  pf_temp_high: 1.5
  use_buffer_prob: 0.5
  replay_buffer_size: 128
  reward_temp_start: 1.0
  reward_temp_end: 2.0
  reward_temp_horizon: 2000
  n_probes: 5
  # --- ADDED: Checkpoint save interval to task config ---
  checkpoint_save_interval: 200 # Default value, can be overridden by sbatch

callbacks:
  - _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/loss"
    mode: "min"
    save_top_k: 1
    dirpath: "checkpoints/"
    filename: "{epoch}-{step}-{val_loss:.3f}"
    every_n_epochs: 1

logger:
  _target_: pytorch_lightning.loggers.WandbLogger
  project: "contradiction-gfn"
  name: null
  group: null # Let wandb handle grouping or set it in the sbatch script
  log_model: false
  mode: offline

hydra:
  job:
    chdir: False