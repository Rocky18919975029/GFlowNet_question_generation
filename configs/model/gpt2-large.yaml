# @package model
# Defines the generative model to be fine-tuned.

name: "gpt2-large"
use_4bit: false # --- CHANGED: Temporarily disable 4-bit quantization for debugging DDP ---

# Configuration for the LoRA adapters from PEFT.
# Hydra will instantiate this class directly.
lora_config:
  _target_: peft.LoraConfig
  r: 16                # LoRA rank
  lora_alpha: 32         # LoRA alpha
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"
  target_modules:
    - "c_attn" # Target attention layers for GPT-2